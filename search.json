[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Genomic Thinking",
    "section": "",
    "text": "Welcome\nThis is the homepage for the AU course Genomic Thinking. You will find all course content here. The Brightspace course page is only used for communication, and assignments.\n\n\nSchedule\n\n1234567891011121314\n\n\n\nWeek 5\nLectures: Course intro and overview: Coop chapt 1, 2, 3 Exercises: Cluster practicals\n\n\n\n\n\nWeek 6\nLectures: The Coalescent and the ARG: Coop chapt 4, Paper: Simons Genome Diversity Project \nExercises: Browsing ARGs\n\n\n\n\n\n\nWeek 7\nReading:\nLectures: Past population demography, HMMs, ARG, Paper on PSMC\nExercises: Estimating past population sizes\n\n\n\n\n\nWeek 8\nLectures: Recombination, Phasing, HMMs, ARG, PSMC Paper on tree inference, Review on recombination rate\nExercises: Phasing and recombination\n\n\n\n\n\nWeek 9\nLectures: Population structure, Incomplete lineage sorting**, HMMs, Coop chapt 6, Review on ILS\nExercises: Inference of population structure and admixture\n\n\n\n\n\nWeek 10\nLectures: Admixture, HMMs, Review on admixture, Paper on admixture inference\nExercises: Tree sequences\n\n\n\n\n\nWeek 11\nLectures: Direct and linked selection, Coop chapt 12, 13, revisit Paper on tree inference\nExercises: Inference of positive selection\n\n\n\n\n\nWeek 12\nLectures: Projects, Popgen overview Mandatory project description\nExercises: GWAS quality control\n\n\n\n\n\nWeek 13\nLectures: GWAS review, Population stratification review, Coop 99-120\nExercises: Association testing\n\n\n\n\n\nWeek 14\nReading:\nLectures: Heritability, Review on heritability and LMM ; Coop Sec 2.2 (p23-36) + Chapt 7 (p119-142)\nExercises: Estimating heritability\n\n\n\n\n\nWeek 15\nLectures: Project\nExercises: Project\n\n\n\n\n\nWeek 16\nLectures: Project\nExercises: SKÆRTORSDAG\n\n\n\n\n\nWeek 17\nLectures: 2. PÅSKEDAG\nExercises: Project\n\n\n\n\n\nWeek 18\nLectures: Project\nExercises: Project\n\n\n\n\n\n\n\n\n\n\nStudent presentations\nWe begin each Monday lecture with two short student presentations. Sign up for three dates in this Google Sheet:\nSchedule for student presentations",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Course content\nThis course introduces key concepts in population genomics, from generating population genetic data sets to the most common population genetic analyses and association studies. The first part focuses on the generation of population genetic data sets. The rest of the course introduces methods for full genome population genetic analysis and their theoretical background. Course topics include analysis of demography, population structure, recombination, selection, and genome-wide association with human health and disease. Description of qualifications",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#objectives-of-the-course",
    "href": "about.html#objectives-of-the-course",
    "title": "About",
    "section": "Objectives of the course",
    "text": "Objectives of the course\nThe participants will after the course have detailed knowledge of the methods and applications required to perform a typical population genomic study.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#learning-outcomes-and-competences",
    "href": "about.html#learning-outcomes-and-competences",
    "title": "About",
    "section": "Learning outcomes and competences",
    "text": "Learning outcomes and competences\nThe participants must at the end of the course be able to:\n\nIdentify an experimental platform relevant to a population genomic analysis\nApply commonly used population genomic methods\nExplain the theory behind common population genomic methods\nReflect on strengths and limitations of population genomic methods\nInterpret and analyze results of population genomic inference\nFormulate population genetics hypotheses based on data.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "weekplan.html",
    "href": "weekplan.html",
    "title": "Week plan",
    "section": "",
    "text": "1\nSchedule for student presentations",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section",
    "href": "weekplan.html#section",
    "title": "Week plan",
    "section": "",
    "text": "Week 5\nLectures: Course intro and overview: Coop chapt 1, 2, 3 Exercises: Cluster practicals",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-1",
    "href": "weekplan.html#section-1",
    "title": "Week plan",
    "section": "2",
    "text": "2\n\nWeek 6\nLectures: The Coalescent and the ARG: Coop chapt 4, Paper: Simons Genome Diversity Project \nExercises: Browsing ARGs",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-2",
    "href": "weekplan.html#section-2",
    "title": "Week plan",
    "section": "3",
    "text": "3\n\nWeek 7\nReading:\nLectures: Past population demography, HMMs, ARG, Paper on PSMC\nExercises: Estimating past population sizes",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-3",
    "href": "weekplan.html#section-3",
    "title": "Week plan",
    "section": "4",
    "text": "4\n\nWeek 8\nLectures: Recombination, Phasing, HMMs, ARG, PSMC Paper on tree inference, Review on recombination rate\nExercises: Phasing and recombination",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-4",
    "href": "weekplan.html#section-4",
    "title": "Week plan",
    "section": "5",
    "text": "5\n\n\nWeek 9\nLectures: Population structure, Incomplete lineage sorting**, HMMs, Coop chapt 6, Review on ILS\nExercises: Inference of population structure and admixture",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-5",
    "href": "weekplan.html#section-5",
    "title": "Week plan",
    "section": "6",
    "text": "6\n\nWeek 10\nLectures: Admixture, HMMs, Review on admixture, Paper on admixture inference\nExercises: Tree sequences",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-6",
    "href": "weekplan.html#section-6",
    "title": "Week plan",
    "section": "7",
    "text": "7\n\nWeek 11\nLectures: Direct and linked selection, Coop chapt 12, 13, revisit Paper on tree inference\nExercises: Inference of positive selection",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-7",
    "href": "weekplan.html#section-7",
    "title": "Week plan",
    "section": "8",
    "text": "8\n\nWeek 12\nLectures: Projects, Popgen overview Mandatory project description\nExercises: GWAS quality control",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-8",
    "href": "weekplan.html#section-8",
    "title": "Week plan",
    "section": "9",
    "text": "9\n\nWeek 13\nLectures: GWAS review, Population stratification review, Coop 99-120\nExercises: Association testing",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-9",
    "href": "weekplan.html#section-9",
    "title": "Week plan",
    "section": "10",
    "text": "10\n\nWeek 14\nReading:\nLectures: Heritability, Review on heritability and LMM ; Coop Sec 2.2 (p23-36) + Chapt 7 (p119-142)\nExercises: Estimating heritability",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-10",
    "href": "weekplan.html#section-10",
    "title": "Week plan",
    "section": "11",
    "text": "11\n\nWeek 15\nLectures: Project\nExercises: Project",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-11",
    "href": "weekplan.html#section-11",
    "title": "Week plan",
    "section": "12",
    "text": "12\n\n\nWeek 16\nLectures: Project\nExercises: SKÆRTORSDAG",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-12",
    "href": "weekplan.html#section-12",
    "title": "Week plan",
    "section": "13",
    "text": "13\n\nWeek 17\nLectures: 2. PÅSKEDAG\nExercises: Project",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "weekplan.html#section-13",
    "href": "weekplan.html#section-13",
    "title": "Week plan",
    "section": "14",
    "text": "14\n\nWeek 18\nLectures: Project\nExercises: Project",
    "crumbs": [
      "Week plan"
    ]
  },
  {
    "objectID": "slides_2026/slides.html",
    "href": "slides_2026/slides.html",
    "title": "Lecture slides",
    "section": "",
    "text": "NB: You need to be logged into Brightspace to download the slides.\n\n\n\nWeek 1\n\nMonday",
    "crumbs": [
      "Lecture slides"
    ]
  },
  {
    "objectID": "exercises/cluster_practicals/README.html",
    "href": "exercises/cluster_practicals/README.html",
    "title": "1  Cluster practicals",
    "section": "",
    "text": "VS Code\nThe exercises in this course will be done using the GenomeDK. If you do not already have one, please use this form to request a user account on the GenomeDK cluster. I the “Reason” field, you can just write “Taking the course in Genomic Thinking”.\nNote that your account on the cluster is temporary. It will be deleted once the course is finished, along with any files you have on the cluster. So make sure to download any files you want to keep before the course is over. Also, your files are not backed up. So if you delete a file, it is gone.\nBefore we get to the cluster we need to get you properly set up on your own machine. Install VS Code if you did not do so already. VS Code has both an editor and a terminal, and looks the same on Windows and Mac. If you have not used a terminal before, or if you are a bit rusty, you should run through this primer before you go on To get a terminal in vscode, press Ctrl+Shift+P (windows) / Cmd+Shift+P (Mac) to open the Command Palette. Then type “Create new terminal”. You will see it auto-completes quite nicely. Select “Terminal: Create new terminal” and press Enter.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster practicals</span>"
    ]
  },
  {
    "objectID": "exercises/cluster_practicals/README.html#connecting-to-the-cluster",
    "href": "exercises/cluster_practicals/README.html#connecting-to-the-cluster",
    "title": "1  Cluster practicals",
    "section": "Connecting to the cluster",
    "text": "Connecting to the cluster\nYou connect to the cluster from the terminal by executing this command (replace username with your cluster user name):\nssh username@login.genome.au.dk\nWhen you do, you are prompted for the password for your cluster username. Enter that and press enter. You are now in your home folder on the cluster. Your terminal looks the same as before but it will print:\n  _____                                ______ _   __\n |  __ \\                               |  _  \\ | / /\n | |  \\/ ___ _ __   ___  _ __ ___   ___| | | | |/ /\n | | __ / _ \\ '_ \\ / _ \\| '_ ` _ \\ / _ \\ | | |    \\\n | |_\\ \\  __/ | | | (_) | | | | | |  __/ |/ /| |\\  \\\n  \\____/\\___|_| |_|\\___/|_| |_| |_|\\___|___/ \\_| \\_/\nIf you run the hostname command, you can see that you are on fe-open-01. Now log out of the cluster again. You do that using exit command or by pressing Ctrl-d. Now you are back on your own machine. Try hostname again and see what your own machine is called. All users have a home folder. However, in this course you will not be using your home folder. We have made a special folder with the same name as your usename in the populationgenomics/students folder. You should keep everything related to the course in this folder. To get from your home folder to the this course folder you just cd:\ncd populationgenomics/students/username\n(replace username with your cluster user name)",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster practicals</span>"
    ]
  },
  {
    "objectID": "exercises/cluster_practicals/README.html#set-up-login-without-password",
    "href": "exercises/cluster_practicals/README.html#set-up-login-without-password",
    "title": "1  Cluster practicals",
    "section": "Set up login without password",
    "text": "Set up login without password\nYou will need to log in to the cluster many many times, so you should set up your ssh connection to the cluster so you can connect securely without typing the password every time. On your own computer, open the terminal of your choice and run the command below. This will generate a private/public key-pair with no password. If you have a key already, you can just use that (the command will warn you if you do).\nssh-keygen -t ed25519 -q -N \"\"\nRun the command below to copy the public key to GenomeDK. You will be asked to enter your password for the cluster.\nssh-copy-id -i ~/.ssh/id_ed25519 &lt;username&gt;@login.genome.au.dk\nYou should now be able to log in to the cluster without typing your password. Running the command below, you should not be prompted for a password.\nssh &lt;username&gt;@login.genome.au.dk",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster practicals</span>"
    ]
  },
  {
    "objectID": "exercises/cluster_practicals/README.html#vs-code-on-the-cluster",
    "href": "exercises/cluster_practicals/README.html#vs-code-on-the-cluster",
    "title": "1  Cluster practicals",
    "section": "VS code on the cluster",
    "text": "VS code on the cluster\nNow that you have this set up, you can use vscode to log in. To set this up, click the icon depicting four squares in the leftmost icon bar. In the panel that expands, use the top search field to search for “Remote Development” and install the extension pack. Once you have done that, you can click the bottom left corner with the two facing arrows and select “Connect to host…” or “Connect current window to host…”, enter username@login.genome.au.dk and press enter. Once vscode has installed some extensions, you are logged in. Now you can open vscode in whatever folder you want. Open populationgenomics/students/username, which is your folder for the course exercises. Once logged into the cluster wih vscode, you an right-click files in the file browser and select “Download”. To upload files to the cluster folder, you can just drag them into the file browser.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster practicals</span>"
    ]
  },
  {
    "objectID": "exercises/cluster_practicals/README.html#install-pixi-on-your-cluster-account",
    "href": "exercises/cluster_practicals/README.html#install-pixi-on-your-cluster-account",
    "title": "1  Cluster practicals",
    "section": "Install Pixi on your cluster account",
    "text": "Install Pixi on your cluster account\nYou need to the Pixi package manager to install the tools you need for each exercise. Log in to the cluster and install pixi by running this command in the vscode terminal:\ncurl -fsSL https://pixi.sh/install.sh | sh",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster practicals</span>"
    ]
  },
  {
    "objectID": "exercises/cluster_practicals/README.html#testing-your-setup",
    "href": "exercises/cluster_practicals/README.html#testing-your-setup",
    "title": "1  Cluster practicals",
    "section": "Testing your setup",
    "text": "Testing your setup\nCheck that your current directory is your personal folder under populationgenomics/\npwd\nMake a folder for your small test project:\nmkdir tester\ncd tester\npixi init\npixi add jupyter nodejs ipykernel seaborn \npixi workspace channel add munch-group\npixi add iplot",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster practicals</span>"
    ]
  },
  {
    "objectID": "exercises/cluster_practicals/README.html#running-commands-in-the-terminal",
    "href": "exercises/cluster_practicals/README.html#running-commands-in-the-terminal",
    "title": "1  Cluster practicals",
    "section": "Running commands in the terminal",
    "text": "Running commands in the terminal\nWhen you log into the cluster you land on the “front-end” of the cluster. The “front-end” is a single machine shared by anyone who log in. You cannot run resource intensive jobs there, but commands that finish in less than ten seconds are ok. For commands that take a long time to run, you need to ask for one of the computing machines on the cluster to run it on instead. Run the command below to request and log in to a computing machine. In this case you ask\nsrun --mem-per-cpu=8g --time=3:00:00 --account=populationgenomics --pty bash\nThay way you will use at most eight gigabyte of memory, that you need at most three hours (the duration of the exercise), and that the computing expenses should be billed to the project populationgenomics (which is our course). When you execute the command your terminal will say “srun: job 40924828 queued and waiting for resources”. That means that you have asked for a machine. Once it prints “srun: job 40924828 has been allocated resources”, you have been logged into a computing node. If you execute the hostname command you will get something like s05n20.genomedk.net. s05n20 is a computing mechine. Now you can execute any command you like without causing trouble for anyone. Now try to log out of the compute node by executing the exit command or by pressing Ctrl-d. If you execute the hostname command again you will get fe1.genomedk.net. fe1 is the front-end. –&gt;",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cluster practicals</span>"
    ]
  },
  {
    "objectID": "exercises/mapping_and_calling/README.html",
    "href": "exercises/mapping_and_calling/README.html",
    "title": "2  Mapping and calling",
    "section": "",
    "text": "Log into the cluster and request a compute node\nAs we learned last week, high-throughput sequencing technologies have in the past few years been producing millions of reads of human genome and other species. To be useful, this genetic information has to be ‘put together’ in a smart way, in the same way as the pieces of a puzzle (reads) need to be mounted according to a picture (reference genome). In this exercise section you will be exposed to different softwares used for mapping reads to a reference sequence and calling variants from the produced alignments. We will use a dataset composed of 28 individuals from 3 different regions: Africa, EastAsia and WestEurasia. You can find the metadata file, containing sample IDs and some extra information for each sample here: ~/populationgenomics/data/metadata/Sample_meta_subset.tsv\nThis dataset is a subset of the Simons Diversity Project (discussed last week).\nLog into the cluster. Then request a machine for your computations. You need five gigabytes (5g) in this exercise so you need to run this command (see also the explanation in the previous exercise):\nRemember to activate your conda environment before running the below commands.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mapping and calling</span>"
    ]
  },
  {
    "objectID": "exercises/mapping_and_calling/README.html#log-into-the-cluster-and-request-a-compute-node",
    "href": "exercises/mapping_and_calling/README.html#log-into-the-cluster-and-request-a-compute-node",
    "title": "2  Mapping and calling",
    "section": "",
    "text": "srun --mem-per-cpu=5g --time=3:00:00 --account=populationgenomics --pty bash",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mapping and calling</span>"
    ]
  },
  {
    "objectID": "exercises/mapping_and_calling/README.html#data-source",
    "href": "exercises/mapping_and_calling/README.html#data-source",
    "title": "2  Mapping and calling",
    "section": "Data source",
    "text": "Data source\nYou will be separated in pairs so you can help each other out with the commands. Each of you will be responsible for 2 individuals and at the end of this exercise we will estimate the mean SNP heterozygosity per individual of the 10 MB region in chromosome 2. You should introduce you results here\nThe following tutorial is based on S_Ju_hoan_North-3.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mapping and calling</span>"
    ]
  },
  {
    "objectID": "exercises/mapping_and_calling/README.html#mapping-reads-against-the-reference",
    "href": "exercises/mapping_and_calling/README.html#mapping-reads-against-the-reference",
    "title": "2  Mapping and calling",
    "section": "Mapping reads against the reference",
    "text": "Mapping reads against the reference\nThe first step when dealing with raw reads is mapping (aligning) them to a reference sequence. For this, we will be using the BWA mapper. BWA stands for Burrows-Wheeler aligner, which allows for fast and accurate alignment of short reads to an indexed reference sequence. Here is the manual. We decided to focus on a 10 MB region of chromosome 2 (from 135MB to 145MB), which contains the lactase gene.\nTwo input files are needed to do genome mapping:\n\nFasta file containing your reference genome Which was downloaded from: (hg19) The path to the fasta file the referense sequence for the human chromosome 2 is here: ~/populationgenomics/fasta/chr2_135_145_subsampl.fa\nThe reads in fastq format, which can be found in this shared folder: ~/populationgenomics/data/fastq/\n\nWe will create a soft-link of fasta reference to your folder, so that we don’t need to type in the full path to the reference everytime we want to use it and avoid copying it to out own directory. Change username to your user account. You might also want to create a separate folder inside your own folder, contianing the exercises for this week. Update the path to your own directory as you see fit.\n    ln -s ~/populationgenomics/fasta/chr2.fa ~/populationgenomics/students/username\nFirst we need to index the reference file for later use. This creates index files used by bwa mem to perform the alignment. To produce these files, run the following command:\n\n    bwa index -p chr2 -a bwtsw chr2.fa \nwhere -p gives the path for the index files produced, while -a bwtsw specifies the indexing algorithm, bwtsw is capable of handling the human genome.\nYou also need to generate a fasta file index. This can be done using samtools:\n\n    samtools faidx chr2.fa\nWe will also create a soft link for the fastq files we want to map to the reference sequence. Please update the path to your own account and samplename with your sample ID. For example, in our case it would be S_Ju_hoan_North-3.\nln -s ~/populationgenomics/data/fastq/samplename.region.fq ~/populationgenomics/students/username\nNow you can map the reads to the reference. This will take around 7 minutes. You can start installing the software that will be used later in this tutorial (IGV) while you wait for it.\n\n    bwa mem -t 16 -p chr2 S_Ju_hoan_North-3.region.fq | samtools sort -O BAM -o S_Ju_hoan_North-3.bam\nThis command is composed of two sub-commands where the output of the “bwa mem” command is piped (“|” is the pipe symbol) into the “samtools sort” command. The output of the “bwa mem” command is an unsorted bam file, which is then used as input into the “samtools sort” command to produce a sorted bam file, which is necessary for further analysis. We could also run the two commands separately, but by using piping we save disc space, as we do not have to save the intermediate unsorted bam file, and altogether speed up the analysis.\nYou can have a look at the bam file generated:\n\n    samtools view S_Ju_hoan_North-3.bam | head\nBam files follow this structure:\n\n\n\nAlt text\n\n\n\nRead name: ID for the given read.\nFlags: Combination of bitwise FLAGs that provide information on how the read is mapped Extra information.\nPosition: Chromosome and position of the first base in the alignment.\nMAPQ: Probability of wrong mapping of the read. It’s in Phred scale, so higher numbers mean lower probabilities: \nCIGAR: summary of the alignment, including start position on the reference sequence, matches, mismatches, deletions and insertions. It may also include information on soft/hard clipping, i.e bases in the 3’ and 5’ ends of teh read that are not part of the alignment. Extra information\nMate information: chromosome and start position of teh read pair, and inferred insert size.\nQuality scores: base qualities of the read.\nMetadata: optional extra information.\n\nFor more information, read this\nYou can get some useful stats of your mapping, by running samtools flagstat:\n\n    samtools flagstat S_Ju_hoan_North-3.bam \nIn order to visualize the alignment of the reads to the reference genome we have just produced, we will use IGV. But first, we need to generate an index file for this software to work. Indexing a genome sorted BAM file allows one to quickly extract alignments overlapping particular genomic regions. Moreover, indexing is required by genome viewers such as IGV so that the viewers can quickly display alignments in each genomic region to which you navigate.\n\n    samtools index S_Ju_hoan_North-3.bam",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mapping and calling</span>"
    ]
  },
  {
    "objectID": "exercises/mapping_and_calling/README.html#downloading-via-terminal",
    "href": "exercises/mapping_and_calling/README.html#downloading-via-terminal",
    "title": "2  Mapping and calling",
    "section": "Downloading via terminal",
    "text": "Downloading via terminal\nYou can download the data via terminal by the following:\n    scp username@login.genome.au.dk:populationgenomics/students/username/S_Ju_hoan_North-3.bam* folder_on_your_computer\nNote that by using the suffix bam* ,we will be downloading all files that start with S_Ju_hoan_North-3.ba, which in this case should be both the BAM and BAI files.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mapping and calling</span>"
    ]
  },
  {
    "objectID": "exercises/mapping_and_calling/README.html#igv-software",
    "href": "exercises/mapping_and_calling/README.html#igv-software",
    "title": "2  Mapping and calling",
    "section": "IGV software",
    "text": "IGV software\nIGV is an Integrative Genomics viewer and can be very useful to look at the results of Mapping and SNP calling. We have not installed it in the cluster, so you can download it to your machine you can go to its website. Three files are necessary to look at this dataset: a reference sequence and the .bam and .bai files, download it from the cluster in a specific directory. Since we are using a human reference, the sequence is already available in the software:\nGo to Genomes —-&gt; Load Genome from server… —-&gt; Filter by human and choose the Human hg19 reference.\nAfter it you will the chromosomes and genes. Now you can download the mapping results by typing: File —-&gt; Load from File… —-&gt; S_Ju_hoan_North-3.bam.\nWhen you zoom in to the lactase region on chromosome 2 (chr2:136,545,410-136,594,750), you will see something like this: \nTry to understand what are the different attributes present in the viewer. If you zoom in very much you will find single nucleotide polymorphisms (SNPs), where the reference sequence does not have the same nucleotide as the data mapped to.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mapping and calling</span>"
    ]
  },
  {
    "objectID": "exercises/mapping_and_calling/README.html#analyzing-read-coverage",
    "href": "exercises/mapping_and_calling/README.html#analyzing-read-coverage",
    "title": "2  Mapping and calling",
    "section": "Analyzing read coverage",
    "text": "Analyzing read coverage\nNow log back into the cluster.\nOne of the attributes one could learn from mapping reads back to the reference is the coverage of reads across the genome. In order to calculate the coverage depth you can use the command samtools depth.\n\n    samtools depth S_Ju_hoan_North-3.bam &gt; S_Ju_hoan_North-3.coverage\nYou can have a look at the resulted file. What do you find in the three different columns?\n    less S_Ju_hoan_North-3.coverage\nLog out of the cluster and fire up a jupyter notebook using slurm-jupyter (see the first exercise for how to do that). For this exercise, specify the memory to 5g (-m 5g). Once jupyter is up and running in your browser, create a new R notebook.\nRun the following code in separate code cells:\nlibrary(ggplot2)\nlibrary(dplyr)\nscaf &lt;- read.table(\"S_Ju_hoan_North-3.coverage\",header=FALSE, sep=\"\\t\", na.strings=\"NA\", dec=\".\", strip.white=TRUE, col.names = c(\"Scaffold\", \"locus\", \"depth\"))\n    \nhead(scaf)\n# Compressing the dataframe in windows\nscaf %&gt;% \nmutate(rounded_position = round(locus, -2)) %&gt;%\n    group_by(rounded_position) %&gt;% \n        summarize(mean_cov = mean(depth)) -&gt; compressed\n\n# Plotting the data\np &lt;- ggplot(data =  compressed, aes(x=rounded_position, y=mean_cov)) + geom_area() + theme_classic() + ylim(0, 400)\n\n# Saving your coverage plot\nggsave(\"S_Ju_hoan_North-3.coverage.pdf\",p)\nWhat are the conclusions you can extract from these analysis? Does the coverage match with what you observed with IGV? Does it match with what you would expect, i.e what you know from the data?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mapping and calling</span>"
    ]
  },
  {
    "objectID": "exercises/mapping_and_calling/README.html#snp-calling",
    "href": "exercises/mapping_and_calling/README.html#snp-calling",
    "title": "2  Mapping and calling",
    "section": "SNP calling",
    "text": "SNP calling\nEven though just a tiny portion (around 2%) of our genomes are based of protein coding regions, this partition contains most of the disease causal variants (mutations), and that is why variant calling is so important from a medical point of view. From the population genetics side of view it is also possible to use these variants to establish differences between individuals, populations and species. It can also be used to clarify the genetic basis of adaptation. These topics will come back to your mind during the following weeks.\nOnce we have mapped our reads we can now start with variant detection. We will use the software Platypus: a tool designed for efficient and accurate variant-detection in high-throughput sequencing data. You can access their website here.\nTo run platypus, we can use this line of code:\n\nplatypus callVariants --bamFile=S_Ju_hoan_North-3.bam --refFile=chr2.fa --output=S_Ju_hoan_North-3.vcf\nThe output will be a single VCF file containing all the variants that Platypus identified, and a ‘log.txt’ file, containing log information.\nLook at the output vcf file. What does the format look like? Does that match with what you observed in the IGV? Download the VCF file to the IGV browser.\nless -S S_Ju_hoan_North-3.vcf \nYou will be using this format further in the course, for now let’s just count the number of heterozygous SNPs in each individual:\ngrep -o '0/1\\|1/0' S_Ju_hoan_North-3.vcf | wc -l\n\n0/0 - the sample is homozygous to the reference (note that these sites usually won’t be listed in single sample vcf files as they are not variants)\n0/1 OR 0/1 - the sample is heterozygous, carrying 1 copy of each of the REF and ALT alleles\n1/1 - the sample is homozygous for the alternate allele\n\nGiven this information you are now able to estimate the mean heterozygosity for your individual of the 10 MB region in chromosome 2.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mapping and calling</span>"
    ]
  },
  {
    "objectID": "exercises/phasing/README.html",
    "href": "exercises/phasing/README.html",
    "title": "3  Genotype phasing",
    "section": "",
    "text": "Log into the cluster and request a compute node\nDuring base calling, we identified the two bases at each position in each diploid individual. However, we do not know which base goes on which of the two chromosomes. That means that we do not know if the two haploid chromosomes look like the left or right example below:\nTo do that we use the program Beagle, which uses a clusering algorithm to call the genotype phase.\nWe put the jointly called bases for Africans, West Eurasians, and East Asians in these three files:\nmake a soft link to your own folder of the files above and this one: ~/populationgenomics/data/genetic_map/plink.chr2.GRCh37.map\nIn this exercise we will just use the Africans and the West Eurasians.\nLog into the cluster. Then request a machine for your computations. You need five gigabytes (5g) in this exercise so you need to run this command (see also the explanation in the previous exercise):",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Genotype phasing</span>"
    ]
  },
  {
    "objectID": "exercises/phasing/README.html#log-into-the-cluster-and-request-a-compute-node",
    "href": "exercises/phasing/README.html#log-into-the-cluster-and-request-a-compute-node",
    "title": "3  Genotype phasing",
    "section": "",
    "text": "srun --mem-per-cpu=5g --time=3:00:00 --account=populationgenomics --pty bash",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Genotype phasing</span>"
    ]
  },
  {
    "objectID": "exercises/phasing/README.html#install-and-activate-todays-environment",
    "href": "exercises/phasing/README.html#install-and-activate-todays-environment",
    "title": "3  Genotype phasing",
    "section": "Install and Activate Todays Environment",
    "text": "Install and Activate Todays Environment\n    conda env create -f ~/populationgenomics/env/exercise_envs/phasing_wk4.yaml\n    conda activate phasing_wk4",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Genotype phasing</span>"
    ]
  },
  {
    "objectID": "exercises/phasing/README.html#running-beagle-without-a-reference-panel",
    "href": "exercises/phasing/README.html#running-beagle-without-a-reference-panel",
    "title": "3  Genotype phasing",
    "section": "Running Beagle Without a Reference Panel",
    "text": "Running Beagle Without a Reference Panel\nFor additional information see the Beagle 4.1 manual\nIn order to obtain phased data, Beagle needs a genetic map. You can find the genetic map for chr2 (hg19 assembly) here: ~/populationgenomics/data/genetic_map/plink.chr2.GRCh37.map\nAfrica:\nbeagle gt=Allvariants_africa.vcf map=plink.chr2.GRCh37.map out=Allvariants_africa_phased\nWest Eurasia\nbeagle gt=Allvariants_westeurasia.vcf map=plink.chr2.GRCh37.map out=Allvariants_westeurasia_phased\nVcf files can both be compressed (gz) or uncompressed. IGV needs it in an uncompressed format, so decompress using\ngunzip -c Allvariants_africa_phased.vcf.gz &gt; Allvariants_africa_phased_t.vcf\nThis command outputs the decompressed to stdout, which then pipes into your file name of choice.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Genotype phasing</span>"
    ]
  },
  {
    "objectID": "exercises/phasing/README.html#visualize-the-phasing",
    "href": "exercises/phasing/README.html#visualize-the-phasing",
    "title": "3  Genotype phasing",
    "section": "Visualize the Phasing",
    "text": "Visualize the Phasing\nbefore we can download the file and visualize it in IGV, we need to create another file, using whatshap\nRun this code below and replace  with the file name you what to visualize.\n    whatshap stats --gtf=&lt;phased&gt;.gtf &lt;phased&gt;.vcf",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Genotype phasing</span>"
    ]
  },
  {
    "objectID": "exercises/phasing/README.html#browsing-the-phased-results",
    "href": "exercises/phasing/README.html#browsing-the-phased-results",
    "title": "3  Genotype phasing",
    "section": "Browsing the phased results",
    "text": "Browsing the phased results\nDownload the phased VCF files and the gft files to your computer and open them in IGV (integrative genomics viewer):\n\nChoose Human hg19 as the reference genome.\nClick File &gt; Load from File... and select you phased VCF file.\n\nExplore phases of haplotypes at two positions in the alignment:\nSelect chr2, zoom all the way in and select find the base at position 136608646. First, take a look at the WestEurasian samples. Consider these questions while zooming further out:\n\nWhat does the haplotypes look like?\nDo you see any long streches of homozygosity?\nWhich haplotypes agree?\nHow wide is the region where they agree?\n\nTo help derive your answers, make use of the metadata file: ~/populationgenomics/data/metadata/Sample_meta_subset.tsv\nNow, compare it with the African samples.\nTry to search the position chr2:136608646 in the UCSC genome browser. Remember we are using the Hg19 assembly version of the reference human genome. Can you find anything that explains your observations? (HINT: https://omim.org/entry/601806#0004)",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Genotype phasing</span>"
    ]
  },
  {
    "objectID": "exercises/arg_and_smc/README.html",
    "href": "exercises/arg_and_smc/README.html",
    "title": "4  Ancestral Recombination Graphs",
    "section": "",
    "text": "Install the ARG dashboard",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ancestral Recombination Graphs</span>"
    ]
  },
  {
    "objectID": "exercises/arg_and_smc/README.html#install-the-arg-dashboard",
    "href": "exercises/arg_and_smc/README.html#install-the-arg-dashboard",
    "title": "4  Ancestral Recombination Graphs",
    "section": "",
    "text": "Plan A: Conda\nCreate a conda environment on your own computer:\nconda create -n arg-dashboard -c conda-forge -c plotly -c kaspermunch popgen-dashboards=1.1.5\nAcitvate the environment:\nconda activate arg-dashboard\nRun the dashboard app:\narg-dashboard\nif it does not show up in your browser, you can right-click (open in new tab) or paste this address into your browser : http://127.0.0.1:8050\n\n\nPlan B: Docker\nInstall Docker Desktop on your own machine.\nStart the Docker Desktop application (you may be prompted to create a login and password for DockerHub)\nRun the dashboard app in your in the Terminal on Mac or in Anaconda PowerShell Prompt on Windows:\ndocker run --rm -i -t -p 8050:8050 kaspermunch/arg-dashboard-linux-amd64:1.1.5\nTo view the dashboard, right-click (open in new tab) or paste this address into your browser : http://127.0.0.1:8050\n\nIf you have a small screen, you may need to zoom out a bit to see en entire dashboard. On Chrome, you click the top right three dots and select a zoom level of 80%.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ancestral Recombination Graphs</span>"
    ]
  },
  {
    "objectID": "exercises/arg_and_smc/README.html#exercise-1-simulate-some-args",
    "href": "exercises/arg_and_smc/README.html#exercise-1-simulate-some-args",
    "title": "4  Ancestral Recombination Graphs",
    "section": "Exercise 1: Simulate some ARGs",
    "text": "Exercise 1: Simulate some ARGs\n\nThe Main panel shows a simulated graph. Click New to generate a new graph. Dropdown menus control the type of graph, the number of samples and the sequence length. Choose “ARG” for the simulation, a sample size of “5” and a sequence length of “2kb”.\nSimulate a lot of ARGS. Just keep clicking New to see the variation of ARGS.\nPick and ARG with 2-3 recombincombination events.\nThe Coalesce and recombination events panel controls the number of events shown in the graph. The Recombination points panel shows the points of recombination in the sequence. Use the Coalesce and recombination events panel to reconstruct the graph step by step by moving the slider from left to right. Make sure you understand what happens at each coalescence and recombination event.\nUse the two sliders in the Recombination points to retrict the view to a smaller part of the sequence. Watch what happens to the arg as you include more or fewer recombination points on the sequence.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ancestral Recombination Graphs</span>"
    ]
  },
  {
    "objectID": "exercises/arg_and_smc/README.html#exercise-2-ancestral-sequences-and-marginal-trees",
    "href": "exercises/arg_and_smc/README.html#exercise-2-ancestral-sequences-and-marginal-trees",
    "title": "4  Ancestral Recombination Graphs",
    "section": "Exercise 2: Ancestral sequences and marginal trees",
    "text": "Exercise 2: Ancestral sequences and marginal trees\n\nClick New to find an ARG with 2-3 recombincombination events where some of the nodes are not red. Ancestral node colors reflect the proportion of the sequence that is represented in the ancestor. Ancestral material is the part of the sequence that has a descendants among the sampled sequences. Mouse-over a node to see the type of event and the proportion of ancestal proportion sequence at each node.\nNotice how mouse-over also activates the Ancestral sequences and Marginal trees panels. The Ancestral sequences panel shows how ancestral material is merged at coalescence events and divided at recombination events. Sequence segments separated by recombination events are shown with separate colors. The genalogy for each segment is shown in the Marginal tree panel with a color matching the segment. Non-ancestral segments are shown in white. Mouse-over the root node of the ARG to see all marginal trees (genealogies) for the ARG. Mouse-over other nodes to see the marginal trees below that node.\nUse the slider in the Recombination points panel to show the graph for only a subset of the sequence. Notice how this affects the shown ARG and the marginal trees when you mouse-over a node. Sequence outside the range specified in the Recombination points panel is shown as gray in the Ancestral sequences panel.\nThe marginal trees look a lot like each other. Try to understand how each one if different from its neighbor. You can make the range in the Recombination points slider really small so it shows only one marginal tree at a time. Now move it along the sequence to see how the ARG changes. Can you see that only one branch detaches and is moved somewhere else?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ancestral Recombination Graphs</span>"
    ]
  },
  {
    "objectID": "exercises/arg_and_smc/README.html#exercise-3-captured-non-ancestral-sequence",
    "href": "exercises/arg_and_smc/README.html#exercise-3-captured-non-ancestral-sequence",
    "title": "4  Ancestral Recombination Graphs",
    "section": "Exercise 3: “Captured” non-ancestral sequence",
    "text": "Exercise 3: “Captured” non-ancestral sequence\n\nSee if you can find any nodes where non-ancestral sequence is “captured” between two ancestral (colored) segments. Figure out how the segment you found got trapped between two ancestral sequence segments.\nSee if you can find it i the SMC. If you cannot, why do you think that is?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ancestral Recombination Graphs</span>"
    ]
  },
  {
    "objectID": "exercises/arg_and_smc/README.html#exercise-4-diamond-recombinations",
    "href": "exercises/arg_and_smc/README.html#exercise-4-diamond-recombinations",
    "title": "4  Ancestral Recombination Graphs",
    "section": "Exercise 4: “Diamond” recombinations",
    "text": "Exercise 4: “Diamond” recombinations\n\nSimulating from the ARG, you may have noticed that some recombinations events produce two lineages that coalesce with each other rather than with other lineages. The following coalescence nullifies the preceeding recombination event. This produces producing an “eye” or “diamond” in the graph. Such recombination events cannot be inferred from data (if you mouse over the nodes you can see why), but they are part of the evolutionary history of a sample none the less.\nTry the SMC. Do you find any “diamonds”?. Why?\nNow try the SMC’. Do you find any “diamonds”?. Why?\nWhat do you think assuming diamonds do not exist does to the estimation of the graph?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ancestral Recombination Graphs</span>"
    ]
  },
  {
    "objectID": "exercises/arg_and_smc/README.html#exercise-5-imagine-complexity",
    "href": "exercises/arg_and_smc/README.html#exercise-5-imagine-complexity",
    "title": "4  Ancestral Recombination Graphs",
    "section": "Exercise 5: Imagine complexity",
    "text": "Exercise 5: Imagine complexity\n\nImagine and ARG for a human chromosome - 100,000 times larger than the 2kb we are considering here.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ancestral Recombination Graphs</span>"
    ]
  },
  {
    "objectID": "exercises/historical_pop_size/README.html",
    "href": "exercises/historical_pop_size/README.html",
    "title": "5  Historical populations size",
    "section": "",
    "text": "UPDATE DOWNLOAD THE .ipynb file to the cluster and either open it in your editor of choice or using SLURM jupyter. There will be commands that you have to run in the commandline and other stuff that you should run in the notebook\nThe Pairwise Sequentially Markovian Coalescent (PSMC) model uses information in the complete diploid sequence of a single individual to infer the history of population size changes. The method was published in 2011 (Li and Durbin 2011) in the paper that you discussed in class. It has become a very popular tool in the world of genomics. In this exercise, we first walk through the steps to generate the necessary input data for PSMC. Then we run PSMC on chromosome 2 of an individual from the Simons Diversity Panel and plot the results.\nFor additional detail on how to run PSMC see the GitHub page for PSMC source code.\nThe bam files and reference genome necessary to run the following scripts can be found at: `/home/Data’.\nThe method you used for base calling in an earlier exercise is state of the art. Unfortunately, to produce the input data for PSMC we cannot just use the base calls or VCF files that we already produced. The first reason is that PSMC required more data than the 10Mb of chromosome 2 that you called bases on. The second reason is that the way you did your base calls do not let us easily produce input data for PSMC, which is a consensus sequence that represents the diploid genome.\nThe example individual used below is a Hungarian individual with id ERR1025630. You can replace that to run the same analysis on another individual.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Historical populations size</span>"
    ]
  },
  {
    "objectID": "exercises/historical_pop_size/README.html#calling-consensus-sequence",
    "href": "exercises/historical_pop_size/README.html#calling-consensus-sequence",
    "title": "5  Historical populations size",
    "section": "Calling consensus sequence",
    "text": "Calling consensus sequence\nStarting from mapped reads, the first step is to produce a consensus sequence in FASTQ format, which stores both the sequence and its corresponding quality scores, that will be used for QC filtering. The consensus sequence has A, T, C or G at homozygous sites, and other letters IUPAC codes to represent heterozygotes. To make the consensus calls, we use the samtools/bcftools suite. We first use samtools mpileup to take the mapped reads and produce a VCF file. We then generate a consensus sequence with bcftools, which we convert to FASTQ (with some additional filtering) by vcfutils.pl. We take advantage of Unix pipes and the ability of samtools to work with streaming input and output to run the whole pipeline (samtools -&gt; bcftools -&gt; vcfutils.pl) as one command. We run our consensus calling pipeline, consisting of a linked set of samtools, bcftools, and vcfutils.pl commands:\nsamtools mpileup -Q 30 -q 30 -u -v -f ~/populationgenomics/data/fasta/chr2.fa -r 2 ~/populationgenomics/data/bam/S_Ami-1.chr2.bam | ~/populationgenomics/software/bcftools call -c | ~/populationgenomics/software/vcfutils.pl vcf2fq -d 5 -D 100 -Q 30 &gt; S_Ami-1.chr2.fq\nThe command takes as input an aligned bam file and a reference genome, generates a summary of the coverage of mapped reads on a reference sequence at a single base pair resolution using samtools mpileup, then calls the consensus sequence with bcftools, and then filters and converts the consensus to FASTQ format. Some parameter explanations:\n\nsamtools:\n\n-Q and -q in mpileup determine the cutoffs for baseQ and mapQ, respectively\n-v tells mpileup to produce vcf output, and -u says that should be uncompressed\n-f is the reference fasta used\n-r is the region to call the mpileup for (in this case, a particular chromosome)\nS_Ami-1.chr2.bam is the bam file to use\n\nbcftools:\n\ncall -c calls a consensus sequence from the mpileup using the original calling method\n\nvcfutils.pl:\n\n-d 5 and -D 100 determine the minimum and maximum coverage to allow for vcf2fq, anything outside that range is filtered\n-Q 30 sets the root mean squared mapping quality minimum to 30\n\n\nThis takes a long to run (about 5-6 hours) so if you get tired of waiting you can get it here:\n~/populationgenomics/data/consensus_fastq/S_Ami-1.chr2.fq\nThere you can also find FASTQ files for all the other individuals we have been working with.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Historical populations size</span>"
    ]
  },
  {
    "objectID": "exercises/historical_pop_size/README.html#creating-a-psmc-input-file",
    "href": "exercises/historical_pop_size/README.html#creating-a-psmc-input-file",
    "title": "5  Historical populations size",
    "section": "Creating a PSMC input file",
    "text": "Creating a PSMC input file\nPSMC takes the consensus FASTQ file, and infers the history of population sizes, but first we need to convert this FASTQ file to the input format for PSMC:\n    fq2psmcfa -q20 S_Ami-1.chr2.fq &gt; S_Ami-1.chr2.psmcfa\nThis transforms the consensus sequence into a fasta-like format where the i-th character in the output sequence indicates whether there is at least one heterozygote in the bin [100i, 100i+100). Have a look at the file using less.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Historical populations size</span>"
    ]
  },
  {
    "objectID": "exercises/historical_pop_size/README.html#running-psmc",
    "href": "exercises/historical_pop_size/README.html#running-psmc",
    "title": "5  Historical populations size",
    "section": "Running PSMC",
    "text": "Running PSMC\nNow we are finally ready to run PSMC. You do that like this:\n    psmc -N50 -t15 -r5 -p \"4+25*2+4+6\" -o S_Ami-1.chr2.psmc S_Ami-1.chr2.psmcfa\nThe command line in the example above has been shown to be suitable for modern humans, inappropiate settings might lead to under/over-fitting. The -p and -t options are used to specify the length and number of time intervals. The -r option is used to specify the initial theta/rho ratio. The -N option sets the maximum number of EM iterations in the fitting of model parameters.\nThis PSMC analysis takes about 25 minutes to complete.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Historical populations size</span>"
    ]
  },
  {
    "objectID": "exercises/historical_pop_size/README.html#plot-your-results",
    "href": "exercises/historical_pop_size/README.html#plot-your-results",
    "title": "5  Historical populations size",
    "section": "Plot your results",
    "text": "Plot your results\nWhen the PSMC completes you can make the PSMC plot. You have to specify the per-generation mutation rate using -u and the generation time in years using -g. To make the plotting script work must first run the following command so the plotting routine knows where to find a file it needs:\nexport GNUPLOT_PS_DIR=~/anaconda3/envs/popgen/share/gnuplot/5.0/PostScript\nThen you can generate the plot like this:\n    psmc_plot.pl -R -u 1.2e-08 -g 25 -p S_Ami-1_plot S_Ami-1.chr2.psmc\nThe -u option specifies the per year mutation rate and the -g the generation time. The -p option specifies the basen name for the output files and -R option preserves the intermediate files the script produces. The latter is handy if you want to make plots yourself combining several PSMC analyses.\nDoes the plot resemble the ones in Li and Durbin. We used a different (more correct) mutation rate than Li et al.. What do you think that does to the plot?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Historical populations size</span>"
    ]
  },
  {
    "objectID": "exercises/historical_pop_size/README.html#compare-individuals-from-different-regions-of-the-world",
    "href": "exercises/historical_pop_size/README.html#compare-individuals-from-different-regions-of-the-world",
    "title": "5  Historical populations size",
    "section": "Compare individuals from different regions of the world",
    "text": "Compare individuals from different regions of the world\nNow compare individuals from different regions. You can find all individuals at:\n\n/home/Data/consensus_files/PSMC\nDo they look different? If so, what do you think could explain this?\nTry to make a plot that show all your results together. Try out the code below:\n\npsmc_data1 &lt;- read.table(\"ERR1025630_sort_dedup_consensus_plot.0.txt\", header=F)\n# you can read data from more than one psmc run:\n# psmc_data2 &lt;- read.table( ...\n# psmc_data3 &lt;- read.table( ...\n\nplot(c(log(1e04), log(1e7)), c(0,3), type=\"n\",  xaxt=\"n\", bty=\"n\", main=\"Results of PSMC\", xlab=\"Years\", ylab='Effective population size', las=1)\nwith(psmc_data1, lines(log(V1), V2, type=\"S\", lw=1.5, col='red'))\n# with(psmc_data2, lines(log(V1), V2, type=\"S\", lw=1.5, col='green'))\n# with(psmc_data3, lines(log(V1), V2, type=\"S\", lw=1.5, col='blue'))\naxis(side=1, at=log((c(1:9*1e4, 1:9*1e5, 1:9*1e6, 1:9*1e7))), labels=F) \nlabs &lt;- c(expression(\"10\"^\"4\"), expression(\"10\"^\"5\"), expression(\"10\"^\"6\"), expression(\"10\"^\"7\"))\ntext(x=log(c(1e4, 1e5, 1e6, 1e7)), y=1.5*par(\"usr\")[3], pos=1, adj=1, labels = labs, xpd = TRUE)\n\n## ggplot version\nlibrary(ggplot2)\npsmc_data1 &lt;- read.table(\"x_sort_dedup_consensus_plot.0.txt\", header=F, col.names = c('Years', 'Effective_pop_size', 'X', 'Y', 'C'))\npsmc_data2 &lt;- read.table(\"y_sort_dedup_consensus_plot.0.txt\", header =F, col.names = c('Years', 'Effective_pop_size', 'X', 'Y', 'C'))\n\n\n# If data 1 is African and data2 is European you can type: \n\npsmc_data1$type = 'African'\npsmc_data2$type = 'European'\n\ndf3 = data.frame(type=c(psmc_data1$type, psmc_data2$type), Years = c(psmc_data1$Years, psmc_data2$Years), 'Effective_pop_size'=c(psmc_data1$Effective_pop_size,psmc_data2$Effective_pop_size))\n# Ggplot version\nlibrary(ggplot2)\n\ng &lt;- ggplot(df3, aes(x=Years, y=Effective_pop_size, color=\"NCL-08\")) + geom_line(aes(color=type), size=1.5) + \n  theme_bw() + \n  labs(x= expression(paste(\"Years (g=25, \", mu, \"=2,5*\", 10^-8,\")\")), y=\"Effective population size\", title='Results of PSMC') +\n  scale_x_log10(breaks=c(1000, 10000, 100000, 1000000), minor_breaks=c(500, 5000, 50000, 500000)) +\n  scale_y_continuous(limits = c(0,3))\ng",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Historical populations size</span>"
    ]
  },
  {
    "objectID": "exercises/population_structure/README.html",
    "href": "exercises/population_structure/README.html",
    "title": "6  Population structure",
    "section": "",
    "text": "Pricipal component analysis (PCA)\nWith the advent of SNP data it is possible to precisely infer the genetic distance across individuals or populations. As written in the book, one way of doing it is by comparing each SNP from each individual against every other individual. This comparison produces the so called: covariance matrix, which in genetic terms means the number of shared polymorphisms across individuals. There are many ways to visualize this data, in this tutorial you will be exposed to Principal Component Analysis and Admixture software.\nWe will use the R package SNPRelate, which can easily handle vcf files and do the PCA. If you want to explore a bit more on the functionality of the package access here.\nWe are going to use these files throughout the exercise:\nNow run slurm-jupyter as your learnt in the first exercise. Once juypter is running, create a new R notebook and run the following code in aseparate code cells:\nto create softlinks from the notebook you can write",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Population structure</span>"
    ]
  },
  {
    "objectID": "exercises/population_structure/README.html#pricipal-component-analysis-pca",
    "href": "exercises/population_structure/README.html#pricipal-component-analysis-pca",
    "title": "6  Population structure",
    "section": "",
    "text": "~/populationgenomics/data/vcf/chr2_135_145_flt.vcf.gz\n~/populationgenomics/data/metadata/sample_infos_accessionnb.csv\n\n\n%%bash\nln -s ~/populationgenomics/data/vcf/chr2_135_145_flt.vcf.gz\nln -s ~/populationgenomics/data/metadata/sample_infos_accessionnb.csv",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Population structure</span>"
    ]
  },
  {
    "objectID": "exercises/population_structure/README.html#if-my-environment-works-you-should-start-by-opening-a-notebook-and-call-these-commands",
    "href": "exercises/population_structure/README.html#if-my-environment-works-you-should-start-by-opening-a-notebook-and-call-these-commands",
    "title": "6  Population structure",
    "section": "if my environment works, you should start by opening a notebook and call these commands",
    "text": "if my environment works, you should start by opening a notebook and call these commands\nimport rpy2\n%load_ext rpy2.ipython\nNow you can jump from python cells to R cells by writing %%R in the start of the cell if you want to run R within that cell. Be aware that you’ll have to write it in each cell you want to execute R within.\n%%R\nlibrary(SNPRelate)",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Population structure</span>"
    ]
  },
  {
    "objectID": "exercises/population_structure/README.html#if-it-didnt-download-the-files-and-run-everthing-in-your-own-r-studio-by-using-thse-commands",
    "href": "exercises/population_structure/README.html#if-it-didnt-download-the-files-and-run-everthing-in-your-own-r-studio-by-using-thse-commands",
    "title": "6  Population structure",
    "section": "if it didnt download the files and run everthing in your own r studio by using thse commands",
    "text": "if it didnt download the files and run everthing in your own r studio by using thse commands\n# Dependencies\nSys.setenv(https_proxy = \"http://proxyserv:3128\", http_proxy = \"http://proxyserv:3128\")\nBiocManager::install(c(\"SNPRelate\"))\n\nlibrary(SNPRelate)\nlibrary(ggplot2)\n%%R #if your are on slurm-jupyter, not if you are on your own computer\n# Reading the metadata information \ninfo &lt;- read.csv(\"sample_infos_accessionnb.csv\", header = T, sep = ';')\n# Setting the directory of the VCF file \nvcf.fn &lt;- \"chr2_135_145_flt.vcf.gz\"\n\n# Transforming the vcf file to gds format\nsnpgdsVCF2GDS(vcf.fn, \"chr2_135_145_flt.gds\", method=\"biallelic.only\")\nThe total number of PCs can be estimated as the minimum between the total number of samples - 1 and the number of predictors.\nn_pcs = min(27-1,49868)\ngenofile &lt;- snpgdsOpen(\"chr2_135_145_flt.gds\",  FALSE, TRUE, TRUE)\npca &lt;- snpgdsPCA(genofile,eigen.cnt=n_pcs)\nsummary(pca)\nQ.1 How many individuals and snps does this dataset have? What is an eigenvector and an eigenvalue?\n    eigenvectors = as.data.frame(pca$eigenvect)\n    colnames(eigenvectors) = as.vector(sprintf(\"PC%s\", seq(1:n_pcs)))\n\n    # Matching the sample names with their origin and population\n    eigenvectors$region = info[match(pca$sample.id, info$ENA.RUN),]$region \n    eigenvectors$population = info[match(pca$sample.id, info$ENA.RUN),]$population\nLet’s first look at how much of the variance of the data is explained by each eigenvector:\n    # Variance proportion:\n    pca_percent &lt;- pca$varprop*100\n\n    qplot(y = pca_percent, x = seq(1, length(pca$eigenval))) + geom_line() + geom_point() + theme_bw() + xlab(\"PC's\") + ylab(\"Variance explained (%)\") \n\nQ.2 How many PC’s do we need in order to explain 50% of the variance of the data? Can you make a cumulative plot of the variance explained PC?\nNow, let’s plot the two first PC’s and color the datapoints by the origin of each individual sample.\nggplot(data = eigenvectors, aes(x = PC1, y = PC2, col = region)) + \n        geom_point(size=3,alpha=0.5) +\n        scale_color_manual(values = c(\"#FF1BB3\",\"#A7FF5B\",\"#99554D\")) +\n        theme_bw()\n\nQ.3 Try to plot PC2 and PC3. Do you see the same patterns? What is the correlation between PC2 and PC3 (hint use the function cor())?\nQ.4 Try also to color the graph based on population. What do you observe?\nNow we will implement LD prunning.\nset.seed(1000)\n\n# This function prune the snps with a thrshold of maximum 0.3 of LD\nsnpset &lt;- snpgdsLDpruning(genofile, ld.threshold=0.3)\nNote: HapMap data uses tag SNPs, i.e SNPs that contain non-redundant information to differentiate between haplotypes.\n# Get all selected snp's ids\nsnpset.id &lt;- unlist(snpset)\npca_pruned &lt;- snpgdsPCA(genofile, snp.id=snpset.id, num.thread=2, ,eigen.cnt=n_pcs)\neigenvectors = as.data.frame(pca_pruned$eigenvect)\ncolnames(eigenvectors) = as.vector(sprintf(\"PC%s\", seq(1:n_pcs)))\n\n# Matching the sample names with their origin and population\neigenvectors$region = info[match(pca_pruned$sample.id, info$ENA.RUN),]$region \neigenvectors$population = info[match(pca_pruned$sample.id, info$ENA.RUN),]$population\n\nggplot(data = eigenvectors, aes(x = PC1, y = PC2, col = region)) + \n        geom_point(size=3,alpha=0.5) +\n        scale_color_manual(values = c(\"#FF1BB3\",\"#A7FF5B\",\"#99554D\")) +\n        theme_bw() + coord_flip()\n\nQ.5 Implement different LD thresholds (0.1, 0.2, 0.3, 0.4, 0.5). How many SNPs are left after each filtering threshold? Are these SNPs linked?\nNow we are going to convert this GDS file into a plink format, to be later used in the admixture exercise:\nsnpgdsGDS2BED(genofile, \"chr2_135_145_flt_prunned.gds\", sample.id=NULL, snp.id=snpset.id, snpfirstdim=NULL, verbose=TRUE)\nUpload the 3 files produced by this last code (chr2_135_145_pruned.gds.bed, chr2_135_145_pruned.gds.bim and chr2_135_145_pruned.gds.fam) to you own folder on the cluster.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Population structure</span>"
    ]
  },
  {
    "objectID": "exercises/population_structure/README.html#log-into-the-cluster-and-request-a-compute-node",
    "href": "exercises/population_structure/README.html#log-into-the-cluster-and-request-a-compute-node",
    "title": "6  Population structure",
    "section": "Log into the cluster and request a compute node",
    "text": "Log into the cluster and request a compute node\nLog into the cluster. Then request a machine for your computations. You need five gigabytes (5g) in this exercise so you need to run this command (see also the explanation in the previous exercise):\nsrun --mem-per-cpu=5g --time=3:00:00 --account=populationgenomics --pty bash",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Population structure</span>"
    ]
  },
  {
    "objectID": "exercises/population_structure/README.html#admixture",
    "href": "exercises/population_structure/README.html#admixture",
    "title": "6  Population structure",
    "section": "Admixture",
    "text": "Admixture\nAdmixture is a program for estimating ancestry in a model based manner from SNP genotype datasets, where individuals are unrelated. The input format required by the software is in binary PLINK (.bed) file. That is why we converted our vcf file into .bed.\nNow with adjusted format and pruned snps, we are ready to run the admixture analysis. We believe that our individuals are derived from three ancestral populations:\nadmixture chr2_135_145_flt_prunned.gds.bed 3\nQ.6 Have a look at the Fst across populations, that is printed in the terminal. Would you guess which populations are Pop0, Pop1 and Pop2 referring to?\nAfter running admixture, 2 outuputs are generated:\n\nQ: the ancestry fractions\nP: the allele frequencies of the inferred ancestral populations\n\nSometimes we may have no priori about K, one good way of choosing the best K is by doing a cross-validation procedure impletemented in admixture as follow:\nfor K in 1 2 3 4 5; \\\n    do admixture --cv chr2_135_145_flt_prunned.gds.bed $K | tee log${K}.out; done\nHave a look at the Cross Validation error of each K:\ngrep -h CV log*.out\nSave it in a text file:\ngrep -h CV log*.out &gt; CV_logs.txt\nLook at the distribution of CV error. You can download your file to your own computer or run it in the cluster using slurm-jupyter.\nCV = read.table('CV_logs.txt')\np &lt;- ggplot(data = CV, aes(x = V3, y = V4, group = 1)) + geom_line() + geom_point() + theme_bw() + labs(x = 'Number of clusters', y = 'Cross validation error')\n\np\n\nQ.7 What do you understand of Cross validation error? Based on this graph, what is the best K?\nPlotting the Q estimates. Choose the K that makes more sense to you.\ntbl = read.table(\"chr2_135_145_flt_prunned.gds.3.Q\")\nord = tbl[order(tbl$V1,tbl$V2,tbl$V3),]\nbp = barplot(t(as.matrix(ord)), \n            space = c(0.2),\n            col=rainbow(3),\n            xlab=\"Individual #\", \n            ylab=\"Ancestry\",\n            border=NA,\n            las=2)\n\n\n\nrplot\n\n\nIn the following part of this exercise you will do both analysis (PCA and Admixture) using a different dataset. The data comes from the HAPMAP Consortium, to learn more about the populations studied in this project access here. The vcf file hapmap.vcf, an information file relationships_w_pops_121708.txt, as well as .bim, .bed, .fam files (only to be used if you get stuck during the exercise) are available for the admixture analysis, this dataset is placed in the cluster, here:\n~/populationgenomics/data/assignment\nAnswer the same questions as answered in this tutorial and write a report (5 pages maximum) about the results and the analysis you have done.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Population structure</span>"
    ]
  },
  {
    "objectID": "exercises/admixture/README.html",
    "href": "exercises/admixture/README.html",
    "title": "7  Historical populations sizes",
    "section": "",
    "text": "Calling consensus sequence\nThe Pairwise Sequentially Markovian Coalescent (PSMC) model uses information in the complete diploid sequence of a single individual to infer the history of population size changes. The method was published in 2011 (Li and Durbin 2011) in the paper that you discussed in class. It has become a very popular tool in the world of genomics. In this exercise, we first walk through the steps to generate the necessary input data for PSMC. Then we run PSMC on chromosome 2 of an individual from the Simons Diversity Panel and plot the results.\nFor additional detail on how to run PSMC see the GitHub page for PSMC source code.\nThe method you used for base calling in an earlier exercise is state of the art. Unfortunately, to produce the input data for PSMC we cannot just use the base calls or VCF files that we already produced. The first reason is that PSMC required more data than the 10Mb of chromosome 2 that you called bases on. The second reason is that the way you did your base calls do not let us easily produce input data for PSMC, which is a consensus sequence that represents the diploid genome.\nThe files we are goint to use are the following: - BAM file: ~/populationgenomics/data/bam/S_Hungarian-2.chr2.bam - BAI file: ~/populationgenomics/data/bam/S_Hungarian-2.chr2.bam.bai - Fasta file: ~/populationgenomics/data/fasta/chr2.fa\nStart by creating soft links to these files in your own folder. The example individual used below is a Hungarian individual with id ERR1025630. You should replace that to run the same analysis on another individual.\nThe code of creating soft links looks like this: ln -s ~/populationgenomics/data/bam/filename_here\nPick a couple of individuals and mark them as yours in here. You will all put your results in a shared folder with the ln -s command:\n~/populationgenomics/shared_results/PSMC\nSo you can all use them when plotting.\nStart by asking for a computing machine by running this command:",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Historical populations sizes</span>"
    ]
  },
  {
    "objectID": "exercises/admixture/README.html#calling-consensus-sequence",
    "href": "exercises/admixture/README.html#calling-consensus-sequence",
    "title": "7  Historical populations sizes",
    "section": "",
    "text": "srun --mem-per-cpu=5g --time=3:00:00 --account=populationgenomics --pty bash",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Historical populations sizes</span>"
    ]
  },
  {
    "objectID": "exercises/admixture/README.html#create-and-activate-psmc-environment",
    "href": "exercises/admixture/README.html#create-and-activate-psmc-environment",
    "title": "7  Historical populations sizes",
    "section": "Create and activate PSMC environment",
    "text": "Create and activate PSMC environment\nconda env create -f ~/populationgenomics/env/exercise_envs/psmc_wk3.yml\nconda activate psmc_wk3\nStarting from mapped reads, the first step is to produce a consensus sequence in FASTQ format, which stores both the sequence and its corresponding quality scores, that will be used for QC filtering. The consensus sequence has A, T, C or G at homozygous sites, and other letters IUPAC codes to represent heterozygotes. To make the consensus calls, we use the samtools/bcftools suite. We first use samtools mpileup to get the pileup of reads for each position. We then generate a consensus sequence with bcftools, which we convert to FASTQ (with some additional filtering) by vcfutils.pl. We take advantage of Unix pipes and the ability of samtools to work with streaming input and output to run the whole pipeline (samtools -&gt; bcftools -&gt; vcfutils.pl) as one command. We run our consensus calling pipeline, consisting of a linked set of samtools, bcftools, and vcfutils.pl commands:\n    ~/populationgenomics/software/bcftools mpileup -Q 30 -Ou -q 30 -f chr2.fa -r 2 S_Hungarian-2.chr2.bam | ~/populationgenomics/software/bcftools call -c | ~/populationgenomics/software/vcfutils.pl vcf2fq -d 5 -D 100 -Q 30 &gt; S_Hungarian-2.chr2.fq\nThe command takes as input an aligned bam file and a reference genome, generates a summary of the coverage of mapped reads on a reference sequence at a single base pair resolution using bcftools mpileup, then calls the consensus sequence with bcftools, and then filters and converts the consensus to FASTQ format. Some parameter explanations:\n\nmpileup:\n\n-Q and -q in mpileup determine the cutoffs for baseQ and mapQ, respectively\n-Ou tells bcftools to output as an intermediate file to pipe to bcftools\n-f is the reference fasta used\n-r is the region to call the mpileup for (in this case, a particular chromosome)\n\nbcftools:\n\ncall -c calls a consensus sequence from the mpileup using the original calling method\n\nvcfutils.pl:\n\n-d 5 and -D 100 determine the minimum and maximum coverage to allow for vcf2fq, anything outside that range is filtered\n-Q 30 sets the root mean squared mapping quality minimum to 30\n\n\nThis takes a long to run (about 5-6 hours) so if you get tired of waiting you can get it here:\n~/populationgenomics/data/consensus_fastq/S_Hungarian-2.chr2.fq\nThere you can also find FASTQ files for all the other individuals we have been working with.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Historical populations sizes</span>"
    ]
  },
  {
    "objectID": "exercises/admixture/README.html#creating-a-psmc-input-file",
    "href": "exercises/admixture/README.html#creating-a-psmc-input-file",
    "title": "7  Historical populations sizes",
    "section": "Creating a PSMC input file",
    "text": "Creating a PSMC input file\nPSMC takes the consensus FASTQ file, and infers the history of population sizes, but first we need to convert this FASTQ file to the input format for PSMC:\n    ~/populationgenomics/software/fq2psmcfa -q20 S_Hungarian-2.chr2.fq &gt; S_Hungarian-2.chr2.psmcfa\nThis transforms the consensus sequence into a fasta-like format where the i-th character in the output sequence indicates whether there is at least one heterozygote in the bin [100i, 100i+100). Have a look at the file using less.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Historical populations sizes</span>"
    ]
  },
  {
    "objectID": "exercises/admixture/README.html#running-psmc",
    "href": "exercises/admixture/README.html#running-psmc",
    "title": "7  Historical populations sizes",
    "section": "Running PSMC",
    "text": "Running PSMC\nNow we are finally ready to run PSMC. You do that like this:\n    ~/populationgenomics/software/psmc -N50 -t15 -r5 -p \"4+25*2+4+6\" -o S_Hungarian-2.chr2.psmc S_Hungarian-2.chr2.psmcfa\nThe command line in the example above has been shown to be suitable for modern humans, inappropiate settings might lead to under/over-fitting. The -p and -t options are used to specify the length and number of time intervals. The -r option is used to specify the initial theta/rho ratio. The -N option sets the maximum number of EM iterations in the fitting of model parameters.\nThis PSMC analysis takes about 25 minutes to complete.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Historical populations sizes</span>"
    ]
  },
  {
    "objectID": "exercises/admixture/README.html#plot-your-results",
    "href": "exercises/admixture/README.html#plot-your-results",
    "title": "7  Historical populations sizes",
    "section": "Plot your results",
    "text": "Plot your results\nWhen the PSMC completes you can make the PSMC plot. You have to specify the per-generation mutation rate using -u and the generation time in years using -g. To make the plotting script work must first run the following command so the plotting routine knows where to find a file it needs:\n    export GNUPLOT_PS_DIR=~/miniconda3/envs/popgen/share/gnuplot/5.0/PostScript\n    export PATH=$PATH:~/populationgenomics/software\nThen you can generate the plot like this:\n    psmc_plot.pl -R -u 1.2e-08 -g 25 S_Hungarian-2.chr2 S_Hungarian-2.chr2.psmc\nThe -u option specifies the per year mutation rate and the -g the generation time. The -R option preserves the intermediate files the script produces. The latter is handy if you want to make plots yourself combining several PSMC analyses.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Historical populations sizes</span>"
    ]
  },
  {
    "objectID": "exercises/admixture/README.html#compare-individuals-from-different-regions-of-the-world",
    "href": "exercises/admixture/README.html#compare-individuals-from-different-regions-of-the-world",
    "title": "7  Historical populations sizes",
    "section": "Compare individuals from different regions of the world",
    "text": "Compare individuals from different regions of the world\nWe want to compare individuals from different regions. Use some of the individuals in the shared folder.\nThe, plot all results together in a R jupyter notebook. Try out the code below:\nlibrary(ggplot2)\n\npsmc_data1 &lt;- read.table(\"S_Hungarian-2.chr2.0.txt\", header=F, col.names = c('Years', 'Effective_pop_size', 'X', 'Y', 'C'))\npsmc_data2 &lt;- read.table(\"S_Ju_hoan_North-3.chr2.0.txt\", header=F, col.names = c('Years', 'Effective_pop_size', 'X', 'Y', 'C'))\n\n# If data 1 is African and data2 is European you can type: \n\npsmc_data1$region = 'European'\npsmc_data2$region = 'African'\n\nd = data.frame(\n                Region = c(psmc_data1$region, psmc_data2$region), \n                Years = c(psmc_data1$Years, psmc_data2$Years), \n                Effective_pop_size = c(psmc_data1$Effective_pop_size,psmc_data2$Effective_pop_size)\n                )\n\nggplot(d, aes(x=Years, y=Effective_pop_size, color=\"NCL-08\")) + \n  geom_line(aes(color=Region), size=1.5) + \n  theme_bw() + \n  labs(x= expression(paste(\"Years (g=25, \", mu, \"=2,5*\", 10^-8,\")\")), y=\"Effective population size\", title='Results of PSMC') +\n  scale_x_log10(breaks=c(1000, 10000, 100000, 1000000), minor_breaks=c(500, 5000, 50000, 500000)) +\n  scale_y_continuous(limits = c(0,3))",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Historical populations sizes</span>"
    ]
  },
  {
    "objectID": "exercises/tree_sequences/README.html",
    "href": "exercises/tree_sequences/README.html",
    "title": "8  Tree sequences",
    "section": "",
    "text": "Request an interactive session on a compute node:\nIf the sequences in your data set are individual haplotypes, it is possible to construct the coalescence trees for each nonrecombining segment of a genomic alignment. If you are working on male X chromosomes this is easy because they are haploid and do not require phasing. However, autosomes are diploid, and we would like to use haplotype-based analysis here as well. This can be done through either phasing of short reads mapped to a reference genome, or by assembing each haplotype de novo using long reads such as PacBio HiFi. Your chr2 data set is already phased so you are good to go.\nIn this exercise we will run the Relate program to infer local trees. In the upcomming exercises we will revisit this tree sequence and use it for inference of demography and selection.\nBegin by requesting an interactive job:",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree sequences</span>"
    ]
  },
  {
    "objectID": "exercises/tree_sequences/README.html#request-an-interactive-session-on-a-compute-node",
    "href": "exercises/tree_sequences/README.html#request-an-interactive-session-on-a-compute-node",
    "title": "8  Tree sequences",
    "section": "",
    "text": "srun --mem-per-cpu=5g --time=3:00:00 --account=populationgenomics --pty bash",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree sequences</span>"
    ]
  },
  {
    "objectID": "exercises/tree_sequences/README.html#set-up-an-environment-for-the-exercise",
    "href": "exercises/tree_sequences/README.html#set-up-an-environment-for-the-exercise",
    "title": "8  Tree sequences",
    "section": "Set up an environment for the exercise:",
    "text": "Set up an environment for the exercise:\nRelate will produce plots with its population size and marginal tree scripts, and this requires some specific r packages. We install these in a separate environment that we just use for running Relate:\n\n\nconda env create -f ~/populationgenomics/env/exercise_envs/bjarke-relate.yml\nAll the Relate scripts can be run in this environment, so make sure the pg-relate is activated when you are working on this exericse. To allow Relate find some files it needs, you also need to run the commands below in order.\nconda activate pg-relate-bjarke\nconda env config vars set LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH\nconda deactivate\nconda activate pg-relate-bjarke",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree sequences</span>"
    ]
  },
  {
    "objectID": "exercises/tree_sequences/README.html#data",
    "href": "exercises/tree_sequences/README.html#data",
    "title": "8  Tree sequences",
    "section": "Data",
    "text": "Data\nThe chr2 data for this exerise is from 60 individuals in the 1000Genomes project. There are 20 individuals from each of the following populations: GBR (British in England and Scotland), JPT (Japanese), YRI (Yoruban).\nCreate some symlinks that point to the following files:\nln -s ~/populationgenomics/data/relate_data/20140520.chr2.strict_mask.fasta\nln -s ~/populationgenomics/data/relate_data/genetic_map_chr2_combined_b37.txt\nln -s ~/populationgenomics/data/relate_data/human_ancestor_2.fa\nln -s ~/populationgenomics/data/relate_data/60_inds.txt\nln -s ~/populationgenomics/data/relate_data/chr2_130_145_phased.vcf.gz\nThis way it looks like the files are in your current folder. You can run ls to see them. The first file is a mask of genomic regions that either have abnormal read depth or contain repetitive elements. The second file is a recombination map. The third file is the ancestral state of every site, based on an alignment with gorilla, chimpanzee and human genomes. The fourth file is a metadata file detailing the population and region for each sample. The last file is the phased genotype VCF file.\nThe documentation for Relate can be found here.\n\nNB: Running Relate below, you should be aware that names of input files are always supplied without the file extensions.\n\nRelate does not accept the standard VCF file format, but instead uses a haps/sample format. You can read up on in the Relate documentation. The authors have been so kind as to supply a script to transform it. First, the vcf is converted to another file format (haplotype file format). If you want to know how it is structured, you can read about it here.\n~/populationgenomics/software/relate/bin/RelateFileFormats --mode ConvertFromVcf --haps chr2.haps --sample chr2.sample -i chr2_130_145_phased\nThen, repetitive (unreliably sequenced) regions must be masked to exclude them form our analysis. We also need to assign each variant as either ancestral or derived using the chimpanzee genome. We do both with this command:\n~/populationgenomics/software/relate/scripts/PrepareInputFiles/PrepareInputFiles.sh --haps chr2.haps --sample chr2.sample --ancestor human_ancestor_2.fa --mask 20140520.chr2.strict_mask.fasta -o prep.chr2\nInspect the generated files.\nQ1: How many SNPs were removed in this filtering step?\nHint: cat chr2.haps | wc -l counts the number of lines in chr2.haps. And zcat is cat for .gz files.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree sequences</span>"
    ]
  },
  {
    "objectID": "exercises/tree_sequences/README.html#build-trees-along-the-genome",
    "href": "exercises/tree_sequences/README.html#build-trees-along-the-genome",
    "title": "8  Tree sequences",
    "section": "Build trees along the genome",
    "text": "Build trees along the genome\nNow, the input is fully prepared, and Relate can be run. This should take less than a minute.\n~/populationgenomics/software/relate/bin/Relate --mode All -m 1.25e-8 -N 30000 --haps prep.chr2.haps.gz --sample prep.chr2.sample.gz --map genetic_map_chr2_combined_b37.txt -o chr2_relate\nQ2: How many SNPs are left per haplotype?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree sequences</span>"
    ]
  },
  {
    "objectID": "exercises/tree_sequences/README.html#estimate-historical-population-size-and-reestimate-branch-lengths-of-trees",
    "href": "exercises/tree_sequences/README.html#estimate-historical-population-size-and-reestimate-branch-lengths-of-trees",
    "title": "8  Tree sequences",
    "section": "Estimate historical population size and reestimate branch lengths of trees",
    "text": "Estimate historical population size and reestimate branch lengths of trees\nThe lengthiest process is this step, in which population size is estimated, and the population size is re-estimates branch lengths. This takes around 20 minutes. While you are waiting, explain to a fellow student how an ARG can be constructed backwards in time and how it can be constructed along the sequence. If time permits, make sure to also explain how the SMC and SMC’ models approximate the ARG.\n~/populationgenomics/software/relate/scripts/EstimatePopulationSize/EstimatePopulationSize.sh -i chr2_relate -m 1.25e-8 --poplabels 60_inds.txt -o popsize --threshold 0\nRelate outputs estimated mutation rate and coalescence times along the region\nUsing the –pop_of_interest flag you can isolate specific populations and see their population size back through time.\nQ3: Look at the files and see what you learn\nWe will revisit this exercise in later sessions. So for now, just have a look at some of the estimated trees to get an impression of what they look like. You can do this using the command below. You specify the position you want to see using the --bp_of_interest option. The command below produces a tree.pdf file showing the tree for position 14000000:\nexport POSITION=14000000 && ~/populationgenomics/software/relate/scripts/TreeView/TreeView.sh --haps chr2.haps --sample chr2.sample --anc popsize.anc --mut popsize.mut --poplabels 60_inds.txt --years_per_gen 28 -o tree_$POSITION --bp_of_interest $POSITION\nTo see the tree files, it is most convenient to work through slurm-jupyter. So you can close your interactive slurm session and run this command to start slurm-jupyter:\nslurm-jupyter -e pg-relate-bjarke -A populationgenomics -m 8g -t 2h -u your_user_name \nOnce you are in, you can start a terminal and activate your pg-relate environment and run the command above. The trees will appear in the file browser to the left where you can double-click to view them.\nQ4: Try to view some trees close to each other and far from each other. Are close trees the same, why?\nQ5: Do trees become more different the further away from each other they are, why?\nQ6: How often do individuals from the same population form a single group? What does that tell you about lineage sorting in humans?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree sequences</span>"
    ]
  },
  {
    "objectID": "exercises/selection/README.html",
    "href": "exercises/selection/README.html",
    "title": "9  Positive selection",
    "section": "",
    "text": "Request an interactive session on a compute node:\nIn this exercise we will work on inference of selection using the Relate tool that we also used in the exercise about tree sequences. In that exercise we analyzed individuals from three populations and inferred all the trees along the chr2 region we have been working on. In this exercise we are going to use those trees to take the analysis a step further and look for positive selection in our chr2 region. So we pick up the tree sequences exercise where we left it and you should continue this exericse in the same folder. That way you have access to the files you you produced the other exercise.\nBegin by requesting an interactive job:\nI will take a while to finish, so just leave the terminal for now and go on with the exercise.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Positive selection</span>"
    ]
  },
  {
    "objectID": "exercises/selection/README.html#request-an-interactive-session-on-a-compute-node",
    "href": "exercises/selection/README.html#request-an-interactive-session-on-a-compute-node",
    "title": "9  Positive selection",
    "section": "",
    "text": "srun --mem-per-cpu=5g --time=3:00:00 --account=populationgenomics --pty bash",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Positive selection</span>"
    ]
  },
  {
    "objectID": "exercises/selection/README.html#build-the-environment-for-the-exercise",
    "href": "exercises/selection/README.html#build-the-environment-for-the-exercise",
    "title": "9  Positive selection",
    "section": "Build the environment for the exercise:",
    "text": "Build the environment for the exercise:\nconda env create -f ~/populationgenomics/env/pg-relate.yml\nMost of you did this last time, I hope, so if it is already installed, just activate it and download some extra packages\nconda activate pg-relate-bjarke\nconda install pandas rpy2",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Positive selection</span>"
    ]
  },
  {
    "objectID": "exercises/selection/README.html#know-what-you-are-doing",
    "href": "exercises/selection/README.html#know-what-you-are-doing",
    "title": "9  Positive selection",
    "section": "Know what you are doing",
    "text": "Know what you are doing\nSince we already have the trees along our genomic alignment we can now use Relate to analyze each tree and compute the likelihood that it was shaped by positive selection. But before running the analysis you need to make sure you understand how Relate quantifies evidence of selelction and how that relates to what you know about positive selection. It just more fun when you know what is happening. So, Team up with one or more fellow students and make sure you understand the relevant Methods section in the Relate paper as well as the section “A tree-based statistic for detecting positive selection” in the supplementary note for the paper.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Positive selection</span>"
    ]
  },
  {
    "objectID": "exercises/selection/README.html#running-relate-on-thee-populations",
    "href": "exercises/selection/README.html#running-relate-on-thee-populations",
    "title": "9  Positive selection",
    "section": "Running Relate on thee populations",
    "text": "Running Relate on thee populations\nWhen you are ready, you should have a command prompt from srun. Use that terminal to activate the pg-relate environment.\nThe Relate command below detects positive selection. At this point, we are detecting selection based on three distinct populations.\n~/populationgenomics/software/relate/scripts/DetectSelection/DetectSelection.sh -i popsize -m 1.25e-8 --poplabels 60_inds.txt -o selection_relate\nHave a look at the selection_relate.sele produced, and see what is in there. Column 35 is the log10 p-value for selection (). Run slurm-jupyter with this command on your own computer:\nslurm-jupyter -e pg-relate -A populationgenomics -m 8g -t 3h -u username\nand open an R notebook (click big blue + button top left and click R notebook). Then this R code to to plot the p-values across the chr2 region:\nIf the R notebook option is not available, you can just make a python notebook and run this command:\nimport rpy2\n\n%load_ext rpy2.ipython\nNow for every cell after this, you should add ‘%%R’ to the start of the cell\nlibrary(tidyverse)\noptions(repr.plot.width=14, repr.plot.height=5)\ndf &lt;- read.table(\"selection_relate.sele\", header=1) %&gt;% select(pos,rs_id,when_mutation_has_freq2) %&gt;% rename(pval_log10 = when_mutation_has_freq2)\nhead(df)\nggplot(df, aes(x=pos, y=-pval_log10, color=when_mutation_has_freq2)) + \n  geom_point(size=1.5) + \n  scale_x_continuous(limits = c(130000000,145000000)) +\n  scale_y_continuous(limits = c(0,7)) +\n  theme_bw()\nNow try to zoom in on the region 135000000-136000000 by changing the arguments to scale_x_continuous. What do you see? It is impossible to read the actual coordinates of each SNP, but you can quickly list the sites with the strongest signal of selection in the terminal: You reverse-sort the result file on column 35 (which is the one with the log10 p-value for selection) using sort -k35r and then pipe the output into head -n 10 which prints the first 10 lines of output:\nsort -k35r selection_relate.sele | head -n 10\nIf you only want columns 1, 2, and 35 you can cut those out like this:\nsort -k35r selection_relate.sele | cut -f 1,2,35 -d ' ' | head -n 10\nDo you see any of the top SNPs in the interval you just looked at?\nTry plotting some of the sites with the following command. Use -o to determine name of the output file, and bp_of_interest for the position (remember to change both every time you run the command).\n~/populationgenomics/software/relate/scripts/TreeView/TreeView.sh --haps chr2.haps --sample chr2.sample --anc popsize.anc --mut popsize.mut --poplabels 60_inds.txt --years_per_gen 28 -o tree_135472847 --bp_of_interest 135472847\nEach command produces a pdf file (tree_135472847.pdf in the above case) that you can view using slurm-jupyter.\nAre the three populations affected by the sweep in the same way?\nDo the three populations form seperate clades? Are they expected to, sweep or not?\nFind the name of the SNP at 135472847 using the command extracting the top SNPs. The name is in column 2 and starts with “rs”.\nUse the UCSC genome browser to look for genes near this SNP. Can you find any good candidates?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Positive selection</span>"
    ]
  },
  {
    "objectID": "exercises/selection/README.html#analysis-of-only-the-british-population-gbr",
    "href": "exercises/selection/README.html#analysis-of-only-the-british-population-gbr",
    "title": "9  Positive selection",
    "section": "Analysis of only the British population (GBR)",
    "text": "Analysis of only the British population (GBR)\nConsider if it using samples from different populations could cause problems for the way Relate detects selection\nIn general, selection scans of this kind will perform better if it is for a single population. Try rerunning the analysis with pure brits. First link these two files to your working directory:\nln -s ~/populationgenomics/data/relate_data/50_GBR_inds.txt\nln -s ~/populationgenomics/data/relate_data/chr2_GBR_phased.vcf.gz \nNow you can rerun all the relate commands with these two new files, including the ones you did in the exercise about tree sequences. All the other files you need are still the same.\nDo you get different or similar results?\nTry plotting trees at the same positions in the two different analyses, and see whether the trees in the analysis of only GBR are similar to the ones including all three populations.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Positive selection</span>"
    ]
  },
  {
    "objectID": "exercises/archaic_humans/README.html",
    "href": "exercises/archaic_humans/README.html",
    "title": "10  Archaic ancestry in humans",
    "section": "",
    "text": "The lengths of Archaic fragments\nThe dataset you will be working in this exercise section was kindly provided by Laurits Skov, the author of the paper you discussed on Monday. He has called archaic fragments using his method in a large number of individuals from the Simons genome diversity project and from the 1000 genomes project, paper here.\nIt is easier to download the dataset and work in your own Rstudio, so i would suggest doing that.\nBut if you insist just do it on the cluster Start up a notebook, and let us explore the dataset (remember to put in your username).\nHere is an example of the code you can use to answer the first set of questions.\nHow many individuals do we have?\nHow many populations do we have?\nHow many regions?\nThe lengths of the fragments are given by the length variable and is regarding the segments that appear to have high density of SNPs after the African SNPs (outgroup) have been filtered away. You will first look at these before classifying them into their most likely archaic origin",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Archaic ancestry in humans</span>"
    ]
  },
  {
    "objectID": "exercises/archaic_humans/README.html#the-lengths-of-archaic-fragments",
    "href": "exercises/archaic_humans/README.html#the-lengths-of-archaic-fragments",
    "title": "10  Archaic ancestry in humans",
    "section": "",
    "text": "Q1. Find the total lengths of Arcahic fragments in each individual.\n\n\nQ2. Find the total lengths of Arcahic fragments in each population.\n\n\nQ3. Which population has longer fragment reads? Why?\n\n\nQ4. What is the length distribution of fragments for the five different regions (hint: you can use facet_grid to plot all at once).\n\n\nQ5. What is the average length of fragments for each population and each region?\n\n\nQ6. What can cause different mean fragment lengths?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Archaic ancestry in humans</span>"
    ]
  },
  {
    "objectID": "exercises/archaic_humans/README.html#the-origin-of-archaic-fragments",
    "href": "exercises/archaic_humans/README.html#the-origin-of-archaic-fragments",
    "title": "10  Archaic ancestry in humans",
    "section": "The origin of archaic fragments",
    "text": "The origin of archaic fragments\nYou can assign individuals fragments to archaic origin using the number of SNPs they share with Denisovans, Altai Neanderthal and Vindija Neanderthal. As a simple first approach, we can assign a fragment to the archaic species with whom shares more SNPs. If there are no SNPs shared with any of the archaics then consider the fragment unassigned.\n\nQ1. For each individual, assign the archaic segments to origin and reconstruct a Figure in the same style as Figure 5 of the Cell paper (plot below).\n\n\n\nQ2. Summarize the results by averaging over region and plot these.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Archaic ancestry in humans</span>"
    ]
  },
  {
    "objectID": "exercises/archaic_humans/README.html#only-if-time-comparison-of-chromosomes",
    "href": "exercises/archaic_humans/README.html#only-if-time-comparison-of-chromosomes",
    "title": "10  Archaic ancestry in humans",
    "section": "Only if time! Comparison of chromosomes",
    "text": "Only if time! Comparison of chromosomes\nYou can also investigate how the introgression events are distributed along the genome.\n\nQ1. Determine the amount of archaic introgression on each chromosome for each of the five regions.\n\n\nQ2. Repeat Q1 with assignment of archaic regions to archaic species.\n\n\nQ3. You will find that the X chromosome is an outlier (compared to a chromosome of a similar size - chr8). How and why?\n\n\nQ4. Combine the Neanderthal fragments for all individuals and plot all the fragments on top of each other along chromosomes (hint use geom_segment() and alpha = 0.02). Can you find “deserts” of archaic admixture and/or evidence for places where Neanderthal or Denisova ancestry has reached very high frequency?\n\n\nQ5. Do you find regions that are devoid of introgression for both the Neanderthal and the Denisovan admixture events?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Archaic ancestry in humans</span>"
    ]
  },
  {
    "objectID": "exercises/GWAS_QC/README.html",
    "href": "exercises/GWAS_QC/README.html",
    "title": "11  GWAS Quality Control",
    "section": "",
    "text": "Software:\nOne of the most important aspects of a GWAS is to do thorough Quality control (QC) of the data before analysing it.\nWe will be using plink 1.9. Plink is a comprehensive tool for handling and analyzing SNP data that can perform many different kinds of analyses. Check the documentation here If you want info about a specific command you can also use help command:",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>GWAS Quality Control</span>"
    ]
  },
  {
    "objectID": "exercises/GWAS_QC/README.html#sample-qc",
    "href": "exercises/GWAS_QC/README.html#sample-qc",
    "title": "11  GWAS Quality Control",
    "section": "Sample QC",
    "text": "Sample QC\n\nIdentification of individuals with discordant sex information\nRemember to start an interactive job:\nsrun --mem-per-cpu=1g --time=3:00:00 --account=populationgenomics --pty bash\nAt the shell prompt, type:\nplink --bfile GWA-data --check-sex --out GWA-QC\nThis command will infer the sex of the sample by looking at the mean homozygosity rate across X-chromosome markers and compare it the sex stated in the .fam file.\n\nTake a look at the output file “GWA-data.sexcheck”. How many problematic samples are there?\n\nProblematic samples can be removed by copying the family ID (FID) and individual ID (IID) of the samples into a text file (e.g. wrong_sex.txt) and using the remove command:\nHINT: To filter for problematic inds, either load it into a notebook or use grep.\nplink --bfile GWA-data --remove wrong_sex.txt --make-bed --out GWA-QC\nThe --out option in plink specifies the prefix of the output files that plink generates. And when we use the –make-bed command we are writing the output to the specified prefix. In this case, all our output files will have the prefix: “GWA-QC”.\n\nEach time a plink command is run it writes a summary to a log file (the file name ends with .log). Look at the log file after removing the problematic individuals. How many cases and controls are left in the data set?\n\n\n\nIdentification of individuals with elevated missing data rates or outlying heterozygosity rate\nAt the shell prompt, type:\nplink --bfile GWA-QC --missing --out GWA-QC\nThis command will create the files GWA-QC.imiss and GWA-QC.lmiss. The fourth column in the file GWA-data.imiss (N_MISS) denotes the number of missing SNPs and the sixth column (F_MISS) denotes the proportion of missing SNPs per individual.\nAt the shell prompt type:\nplink --bfile GWA-QC --het --out GWA-QC \nThis command will create the file GWA-data.het, in which the third column denotes the observed number of homozygous genotypes [O(Hom)] and the fifth column denotes the number of non-missing genotypes [N(NM)] per individual.\nYou can calculate the observed heterozygosity rate per individual using the formula:\nHet = (N(NM) − O(Hom))/N(NM)\n\nOpen a R jupyter notebook and create a plot in which the observed heterozygosity rate per individual is plotted on the x axis and the proportion of missing SNPs per individuals is plotted on the y axis. Interpret your results.\n\nHint: You can merge the two tables by running something like this:\nlibrary(dplyr)\nd_miss &lt;- read.table(\"GWA-QC.imiss\",header=T)\nd_het &lt;- read.table(\"GWA-QC.het\",header=T)\nd &lt;- inner_join(d_miss,d_het)\nWe will filter out outliers for either of the variables, and then save the file like this:\nwrite.table(d_m, file = \"wrong_het_missing_values.txt\", col.names = F, row.names = F)\n\nMake a file with the FID and IID of all individuals that have a genotype missing rate &gt;=0.03 or a heterozygosity rate that is more than 3 s.d. from the mean. Then use plink to remove these individuals from the data set.\n\nplink --bfile GWA-QC --remove wrong_het_missing_values.txt --make-bed --out GWA-QC\nNote that by providing the same prefix and running –make-bed, we will overwrite our bed/bim/fam files. This is OK in the context of this exercise and helps simplifying things, but can be dangerous in real life.\n\n\nIdentification of duplicated or related individuals\nTo identify duplicated or related individuals we will calculate the identity by descent (IBD) matrix. This works best if it is done on a set of non-correlated SNPs. So first we will “prune” the data and create a list of SNPs where no pair (within a given genomic interval) has an r2 value greater than a given threshold, typically chosen to be 0.2. This can be done by the indep-pairwise command, using 500kb as window size and 5 variants as step size:\nplink --bfile GWA-QC --indep-pairwise 500kb 5 0.2 --out GWA-QC\nIt saves the list of independent SNPs as GWA-QC.prune.in (This data set was simulated without LD so in this case there will not be a lot of variants removed.) To calculate IBD between each pair of individuals, type the following command at the shell prompt:\nplink --bfile GWA-QC --extract GWA-QC.prune.in --genome --min 0.185 --out GWA-QC\nThe --min 0.185 option means that it will only print the calculated IBD if it is above 0.185 (Mean between second-degree relatives:0.25 and third-degree relatives:0.125). The PI_HAT value in column 10 of the output file will be a number between 0 and 1 saying how much of the genome the two individuals share (1 for identical twins, 0.5 for siblings etc.). This command will produce a file called GWA-data.genome .\n\nRemove a member from each of the pairs that are too closely related from the data set. To keep it simple you can just always remove the individual mentioned first.\n\nHint: Open the GWA-QC.genome file in your jupyter notebook and obtain unique IDs with something of the type:\nibd &lt;- read.table('GWA-QC.genome', header = TRUE)\nmembers &lt;- ibd$FID1\nmembers &lt;- unique(members)\nwrite.table(cbind(members,members), file = 'wrong_ibd.txt', col.names = F, row.names = F)\nHere we are exploiting the fact that FID and IID are the same in our dataset, but we would have to contition on both if that wasn’t the case.\nTo remove these individuals, we will use again the –remove parameter and create updated bed/bim/fam files:\nplink --bfile  GWA-QC --remove wrong_ibd.txt --make-bed --out GWA-QC",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>GWAS Quality Control</span>"
    ]
  },
  {
    "objectID": "exercises/GWAS_QC/README.html#snp-qc",
    "href": "exercises/GWAS_QC/README.html#snp-qc",
    "title": "11  GWAS Quality Control",
    "section": "SNP QC",
    "text": "SNP QC\n\nSNPs with an excessive missing data rate\nRun the --missing command again to generate the GWA-data.lmiss with the missing data rate for each SNP.\n\nUse R to make a histogram of the missing data rates (F_MISS).\n\nThe --test-missing command tests for association between missingness and case/control status, using Fisher’s exact test. It produces a file with “.missing” suffix.\n\nRun the test-missing command and make a list of all the names of all SNPs where the differential missingness p-value is less than 1e-5. Save the list as fail-diffmiss-qc.txt.\n\nTo remove low-quality SNPs, type the following command at the shell prompt:\nplink --bfile GWA-QC --exclude fail-diffmiss-qc.txt --geno 0.05 --hwe 0.00001 --maf 0.01 --make-bed --out GWA-QC\nIn addition to removing SNPs identified with differential call rates between cases and controls, this command removes SNPs with call rate less than 95% with --geno option and deviation from HWE (p&lt;1e-5) with the --hwe option. It also removes all SNPs with minor allele frequency less than a specified threshold using the --maf option.\n\nHow many SNPs are left in the clean data set?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>GWAS Quality Control</span>"
    ]
  },
  {
    "objectID": "exercises/GWAS_association/README.html",
    "href": "exercises/GWAS_association/README.html",
    "title": "12  Association testing",
    "section": "",
    "text": "Software:\nWe will be using plink 1.9, you can see the documentation here. We will also be using R and Rstudio to make plots and make simple calculations.\n\n\nData:\nWe will use a simulated date set of 47 cases and 41 controls. You can find it in:\n~/populationgenomics/data/GWAS/GWAS_test/gwa.bim\n~/populationgenomics/data/GWAS/GWAS_test/gwa.bed\n~/populationgenomics/data/GWAS/GWAS_test/gwa.fam\n\n\nExercise contents:\nIn this practical, we will go through the steps of performing association tests in plink and adjusting for principle components.\n\n\nTest for association with disease status using a Fisher’s exact test\nTo test for association between SNPs and disease status using an allelic Fisher’s exact test, type the following command at the shell prompt:\nplink --bfile gwa --assoc fisher --out gwa\n1) Take a look at the output file “gwa.assoc.fisher”. What is the p-value and location of the most significant variant?\n2) Is the most significant variant significant if you do Bonferroni correction?\nOther multiple testing corrections:\nplink --bfile gwa --assoc fisher --adjust --out gwa\n\n\nMake plots\nWe will use the R package “qqman” to make Manhattan plots and qq-plots. The package is available in CRAN so it can be installed using the “install.packages” command. You can read in the association results and make a Manhattan plot by typing:\nd &lt;- read.table('gwa.assoc.fisher', head=T)\nmanhattan(d)\n3) Are there other variants close to the most significant variant that are also associated with the disease?\nTo make a QQ-plot you should use the “qq” function:\nqq(d$P)\n4) Is there a general inflation of the test statistic?\n\n\nGenomic Control.T\nThe inflation factor (𝝺) can be calculated as the median of the Chi-squared statistics computed divided by the median of the Chi-squared distribution under the null. Given a p-value (p) the corresponding Chi-squared quantile can be calculated as:\nqchisq(p, df=1, lower.tail = F)\n5) What is the inflation factor?\nTo do genomic control (to adjust for inflated test statistic) you divide the Chi-squared values by the inflation factor. To turn a Chi-squared quantile (q) into a p-value you use the “pchisq” function:\npchisq(q, df=1, lower.tail = F)\n6) What is the p-value of the most significant marker after genomic control?\n\n\nPCA\nIt is best to perform the PCA on a LD-pruned set of SNPs:\nplink --bfile gwa --indep-pairwise 500kb 5 0.2 --out gwa\nTo use the pruned set of SNPs to calculate the relationship matrix and calculate the first 20 principle components (PCs) type:\nplink --bfile gwa --extract gwa.prune.in --pca 20 --out gwa\nThis calculates the eigenvalues and the eigenvectors, and stores them in two files (gwa.eigenval, gwa.eigenvec).\n7) Load gwa.eigenvec into R and make a plot with the first PC on the x-axis and the second PC on the y-axis. Does it look like there is population structure in the data? How many populations?\nUse the eigenvalues to compute the variance explained by each PC.\n8) How large a percentage of the variance does the first PC approximately explain?\n\n\nAdjusting for PCs\nWe can use a logistic regression test to perform an association test while correcting for covariates. To include the first PC as a covariate type:\nplink --bfile gwa --logistic --covar gwa.eigenvec --covar-number 1\nThe resulting file “plink.assoc.logistic” contains p-values for both the SNPs and the covariates. To get the p-values for the SNPs should look at the rows with the value “ADD” in the “TEST” column. (It is possible to include more PCs. To include the first x covariates you can write “–covar-number 1-x”.)\n9) Create Manhattan plot and QQ-plot for the new results. Does the QQ-plot look better?\n10) What is the inflation factor now?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Association testing</span>"
    ]
  },
  {
    "objectID": "exercises/heritability/README.html",
    "href": "exercises/heritability/README.html",
    "title": "13  Heritability",
    "section": "",
    "text": "Software:\nIn this exercise we will be using GCTA. You can see the documentation here. We will be also using plink 1.9, you can see the documentation here.\n\n\nExercise contents:\nWe will estimate the amount of variance explained by the SNPs in a GWAS dataset. You can find the data here:\n~/populationgenomics/data/GWAS/GWAS_heritability/gwa.bim\n~/populationgenomics/data/GWAS/GWAS_heritability/gwa.bed\n~/populationgenomics/data/GWAS/GWAS_heritability/gwa.fam\n~/populationgenomics/data/GWAS/GWAS_heritability/gwa.phen\n\n\nCalculating the genetic relationship matrix\nWe will use plink to calculate the genetic relationship matrix (GRM) since it is faster than gcta. At the shell prompt, type:\nplink --make-grm-gz --bfile gwa --out gwa\nThis will save the genetic relationship matrix in the zipped file gwa.grm.gz. Try to read it into R:\nd &lt;- read.table(gzfile('gwa.grm.gz'))\nnames(d) &lt;- c(\"sample1\",\"sample2\",\"variants\",\"statistic\")\n1) If you exclude the lines where an individual is compared to itself (column 1 is equal to column 2) what is the highest value in the GRM then?\n\n\nEstimating variance components\nWe’ll first need to install the package in our counda environments by running:\nconda install -c bioconda gcta\n\n\nEstimating variance components\nWe can use gcta to estimate how much of the variance in the phenotype in gwa.phen is explained by the SNPs:\ngcta64 --grm-gz gwa --pheno gwa.phen --reml --out test\n2) How much of the phenotypic variance (Vp) is explained by the genetic variance (V(G))?\n3) Is this number larger or smaller than the narrow-sense heritability (h^2) of the trait?\n\n\nEstimating variance components for groups of SNPs\nThe estimation of variance components can be used to answer questions about how much of the heritability is explained by different parts of the genome (for example different chromosomes or different functional annotations).\n4) How much of the phenotypic variance can be explained by the genetic variants on chromosome 6? (You can use the “–chr” flag in plink to build a GRM only using variants from a particular chromosome)\n5) Does chromosome 6 contribute more to the heritability than would be expected? How many of the genetic variants in the data set are located on chr 6? (you can use the genetic map in gwa.bim to see the location of the variants).",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Heritability</span>"
    ]
  }
]